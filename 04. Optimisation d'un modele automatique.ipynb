{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3aeaf3f-8f6a-450a-875c-f97d76ad4e31",
   "metadata": {},
   "source": [
    "# Optimisation d'un modèle automatique étape par étape\n",
    "___\n",
    "\n",
    "Dans ce cours, nous verrons un autre exemple concret sur comment optimiser un modèle automatiquement par descente de gradient avec le package Autograd de PyTorch. Cet exemple sera aussi sur un modèle de régression linéaire.\n",
    "Ce cours est séparé en 4 étapes distinctes :\n",
    "\n",
    "    - Étape 1 : Implémentation manuelle. Dans la première étape, nous allons implémenter le modèle de régression linéaire dans Python.\n",
    "            Prediction : Manuel\n",
    "            Calcul de gradient : Manuel\n",
    "            Calcul du regret : Manuel\n",
    "            Mise à jour des paramètres : Manuel\n",
    "    \n",
    "    - Étape 2 : Nous allons voir comment automatiser la descente de gradient via le package Autograd de PyTorch\n",
    "            Prediction : Manuel\n",
    "            Calcul de gradient : Autograd\n",
    "            Calcul du regret : Manuel\n",
    "            Mise à jour des paramètres : Manuel\n",
    "            \n",
    "    - Étape 3 : Nous allons voir comment automatiser ensuite le calcul du regret L et la mise à jour des paramètres avec PyTorch\n",
    "            Prediction : Manuel\n",
    "            Calcul de gradient : Autograd\n",
    "            Calcul du regret : PyTorch Loss\n",
    "            Mise à jour des paramètres : PyTorch Optimizer\n",
    "            \n",
    "    - Étape 4 : Nous allons implémenter un modèle de régression linéaire fourni par PyTorch \n",
    "            Prediction : PyTorch Model\n",
    "            Calcul de gradient : Autograd\n",
    "            Calcul du regret : PyTorch Loss\n",
    "            Mise à jour des paramètres : PyTorch Optimizer\n",
    "            \n",
    "## Étape 1 : Implémentation manuelle\n",
    "\n",
    "Nous allons utiliser uniquement la librairie Numpy pour cette étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3078b37b-9c36-4eee-a903-ef69d7fda231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c17cb587-8ea1-45b2-9f44-777769ae065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0ebc422-8daa-44ba-bf1f-9377d621663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a5cb533-878e-406e-8cfc-155e0fa80241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction de modèle\n",
    "\n",
    "def avant(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10451dff-1739-433b-8aa3-01b57e3c4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du regret : Erreur quadratique moyenne de la régression linéaire\n",
    "\n",
    "def regret(y, y_predit):\n",
    "    return ((y_predit-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88250503-84d2-427d-9ae2-3e4d76d764e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de gradient : EQM = 1/N * (w*x - y)²\n",
    "# dL/dw = 1/N * 2x * (w*x - y)\n",
    "\n",
    "def gradient(x, y, y_predit):\n",
    "    return np.dot(2*x, y_predit-y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b739bf75-6122-4918-8bb3-20fab552dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction avant apprentissage : f(5) = 0.000\n",
      "1: w = 1.200, regret = 30.00000000\n",
      "2: w = 1.680, regret = 4.79999924\n",
      "3: w = 1.872, regret = 0.76800019\n",
      "4: w = 1.949, regret = 0.12288000\n",
      "5: w = 1.980, regret = 0.01966083\n",
      "6: w = 1.992, regret = 0.00314570\n",
      "7: w = 1.997, regret = 0.00050332\n",
      "8: w = 1.999, regret = 0.00008053\n",
      "9: w = 1.999, regret = 0.00001288\n",
      "10: w = 2.000, regret = 0.00000206\n",
      "11: w = 2.000, regret = 0.00000033\n",
      "12: w = 2.000, regret = 0.00000005\n",
      "13: w = 2.000, regret = 0.00000001\n",
      "14: w = 2.000, regret = 0.00000000\n",
      "15: w = 2.000, regret = 0.00000000\n",
      "16: w = 2.000, regret = 0.00000000\n",
      "17: w = 2.000, regret = 0.00000000\n",
      "18: w = 2.000, regret = 0.00000000\n",
      "19: w = 2.000, regret = 0.00000000\n",
      "20: w = 2.000, regret = 0.00000000\n",
      "Prédiction après apprentissage : f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "print(f'Prédiction avant apprentissage : f(5) = {avant(5):.3f}')\n",
    "\n",
    "# Apprentissage \n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for i in range(n_iters):\n",
    "    # Prédiction = lecture avant\n",
    "    y_pred = avant(X)\n",
    "    \n",
    "    # Regret \n",
    "    L = regret(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if i % 1 ==0:\n",
    "        print(f'{i+1}: w = {w:.3f}, regret = {L:.8f}')\n",
    "\n",
    "print(f'Prédiction après apprentissage : f(5) = {avant(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128811f-66a1-4308-9e46-1be74f7af293",
   "metadata": {},
   "source": [
    "Avant l'apprentissage, on voit que la prédiction de f(5) est de 0, ce qui est normal car _w0_ = 0. Lors de l'apprentissage, on voit que _w_ tend vers 2 et que le regret _L_ tend vers 0, ce qui est tout à fait ce que nous attendons car _Y = 2X_ et on veut minimiser L. On remarque aussi que la prédiction après apprentissage est de 10. Cette méthode d'apprentissage manuelle est donc efficace et marche correctement, mais elle est assez lourde à mettre en place si on prend un modèle plus conséquent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270323a3-6b15-4740-b04e-5d32eb54f4db",
   "metadata": {},
   "source": [
    "## Etape 2 : Descente de gradient automatique\n",
    "\n",
    "Cette fois-ci nous n'utilisons par Numpy mais la librairie PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5ef3933-1d48-4f22-92fc-ddb54b605e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1eae7db0-1ac3-4975-bc09-01e432adbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9559633c-749f-4872-bf25-ea8b1764e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d028447d-38d6-4d06-84c7-753a2d4a0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction de modèle\n",
    "\n",
    "def avant(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "880c797d-a546-4bba-ad25-49637a65258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du regret : Erreur quadratique moyenne de la régression linéaire\n",
    "\n",
    "def regret(y, y_predit):\n",
    "    return ((y_predit-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81da0440-d273-462a-bd00-d189cfe2f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction avant apprentissage : f(5) = 0.000\n",
      "1: w = 0.300, regret = 30.00000000\n",
      "8: w = 1.455, regret = 3.08308983\n",
      "15: w = 1.825, regret = 0.31684780\n",
      "22: w = 1.944, regret = 0.03256231\n",
      "29: w = 1.982, regret = 0.00334642\n",
      "36: w = 1.994, regret = 0.00034392\n",
      "43: w = 1.998, regret = 0.00003534\n",
      "50: w = 1.999, regret = 0.00000363\n",
      "57: w = 2.000, regret = 0.00000037\n",
      "64: w = 2.000, regret = 0.00000004\n",
      "71: w = 2.000, regret = 0.00000000\n",
      "Prédiction après apprentissage : f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "print(f'Prédiction avant apprentissage : f(5) = {avant(5):.3f}')\n",
    "\n",
    "# Apprentissage \n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 71\n",
    "\n",
    "for i in range(n_iters):\n",
    "    # Prédiction = lecture avant\n",
    "    y_pred = avant(X)\n",
    "    \n",
    "    # Regret \n",
    "    L = regret(Y, y_pred)\n",
    "    \n",
    "    # Gradients = rétropropagation\n",
    "    L.backward() # dL/dw\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # Vider le gradient\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if i % 7 == 0:\n",
    "        print(f'{i+1}: w = {w:.3f}, regret = {L:.8f}')\n",
    "\n",
    "print(f'Prédiction après apprentissage : f(5) = {avant(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319c8c5-14b7-4c0c-bbed-8b13418f51aa",
   "metadata": {},
   "source": [
    "Avec Autograd, on remarque que l'apprentissage demande plus d'itérations que manuellement, et cela est dû au fait que la rétropropagation n'effectue pas le même calcul que nous avons fait précédemment. \n",
    "\n",
    "## Étape 3 : Automatisation de l'optimisation et du calcul de L\n",
    "\n",
    "Ici, nous allons importer en plus de la librarie PyTorch son module de réseau de neurones \"nn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a553184-04d0-4ce3-9700-755ca8b81127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ad33258-ad35-41a1-b491-95ca197958e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dca615d3-7159-49a5-aeb7-1ae5f8af9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc1d0de4-8147-4e8e-a47b-c4343175254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction de modèle\n",
    "\n",
    "def avant(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a6f3048-f8c2-40bc-85ab-6112d4840756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction avant apprentissage : f(5) = 0.000\n",
      "1: w = 0.300, regret = 30.00000000\n",
      "8: w = 1.455, regret = 3.08308983\n",
      "15: w = 1.825, regret = 0.31684780\n",
      "22: w = 1.944, regret = 0.03256231\n",
      "29: w = 1.982, regret = 0.00334642\n",
      "36: w = 1.994, regret = 0.00034392\n",
      "43: w = 1.998, regret = 0.00003534\n",
      "50: w = 1.999, regret = 0.00000363\n",
      "57: w = 2.000, regret = 0.00000037\n",
      "64: w = 2.000, regret = 0.00000004\n",
      "71: w = 2.000, regret = 0.00000000\n",
      "Prédiction après apprentissage : f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "print(f'Prédiction avant apprentissage : f(5) = {avant(5):.3f}')\n",
    "\n",
    "# Apprentissage \n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 71\n",
    "\n",
    "regret = nn.MSELoss() # MSE = EQM\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    # Prédiction = lecture avant\n",
    "    y_pred = avant(X)\n",
    "    \n",
    "    # Regret \n",
    "    L = regret(Y, y_pred)\n",
    "    \n",
    "    # Gradients = rétropropagation\n",
    "    L.backward() # dL/dw\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Vider le gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if i % 7 == 0:\n",
    "        print(f'{i+1}: w = {w:.3f}, regret = {L:.8f}')\n",
    "\n",
    "print(f'Prédiction après apprentissage : f(5) = {avant(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294b3fa-e1a6-414b-9b2e-4a4048c24a76",
   "metadata": {},
   "source": [
    "## Étape 4 : Prédiction par un modèle PyTorch\n",
    "\n",
    "Ici, nous n'avons pas besoin d'implémenter manuellement _w_ ni la prédiction du modèle, on utilise la libraire PyTorch et ses modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "700d8f41-3374-42dc-bc26-043a4899498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13849253-81da-4039-87cb-3f9bec8ec41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y_train = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "32c49557-05dc-4c3f-b7fc-37012d76f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a43324e-b8e9-4c97-b095-fe4c8c6f1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "98cfa958-1b50-4451-99ba-f6c282954a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e95ecb2d-e941-4a1b-8a1c-b16c8edecc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction avant apprentissage : f(5) = 2.834\n",
      "1: w = 0.611, regret = 13.35665131\n",
      "101: w = 1.703, regret = 0.12788254\n",
      "201: w = 1.780, regret = 0.07020593\n",
      "301: w = 1.837, regret = 0.03854224\n",
      "401: w = 1.879, regret = 0.02115924\n",
      "501: w = 1.911, regret = 0.01161615\n",
      "601: w = 1.934, regret = 0.00637713\n",
      "701: w = 1.951, regret = 0.00350099\n",
      "801: w = 1.964, regret = 0.00192199\n",
      "901: w = 1.973, regret = 0.00105515\n",
      "1001: w = 1.980, regret = 0.00057926\n",
      "Prédiction après apprentissage : f(5) = 9.959\n"
     ]
    }
   ],
   "source": [
    "print(f'Prédiction avant apprentissage : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Apprentissage \n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 1001\n",
    "\n",
    "regret = nn.MSELoss() # MSE = EQM\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    # Prédiction = lecture avant\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Regret \n",
    "    L = regret(Y, y_pred)\n",
    "    \n",
    "    # Gradients = rétropropagation\n",
    "    L.backward() # dL/dw\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Vider le gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'{i+1}: w = {w[0][0].item():.3f}, regret = {L:.8f}')\n",
    "\n",
    "print(f'Prédiction après apprentissage : f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcfe8f-08b7-484c-816f-6372fb04b8f2",
   "metadata": {},
   "source": [
    "Comme l'initialisation est aléatoire et réalisée par PyTorch, notre modèle demande beaucoup plus d'itérations pour pouvoir être \"parfait\". Mais si jamais PyTorch ne possède pas le modèle que l'on veut, nous pouvons créer nous-même un modèle que PyTorch peut ensuite exploiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c5d1881b-ee95-4413-b617-e3efd79ae2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # On définit les couches\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def avant(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc77d56-464c-49a1-9367-852f81bd07cb",
   "metadata": {},
   "source": [
    "C'est un exemple bateau car le modèle existe déjà mais vous pouvez créer le modèle que vous voulez (si jamais vous héritez des modèles PyTorch)\n",
    "\n",
    "______\n",
    "\n",
    "C'est la fin de ce cours, merci d'avoir été attentif, nous nous retrouverons donc au prochain cours pour parler de différents modèles existant dans PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dc49b-8877-489c-8131-7935783f02d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
